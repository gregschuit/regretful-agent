{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De [link](https://github.com/chihyaoma/regretful-agent/tree/master/tasks/R2R-pano)\n",
    "\n",
    "Each JSON Lines entry contains a guide annotation for a path in the environment.\n",
    "\n",
    "Data schema:\n",
    "\n",
    "```python\n",
    "{'split': str,\n",
    " 'instruction_id': int,\n",
    " 'annotator_id': int,\n",
    " 'language': str,\n",
    " 'path_id': int,\n",
    " 'scan': str,\n",
    " 'path': Sequence[str],\n",
    " 'heading': float,\n",
    " 'instruction': str,\n",
    " 'timed_instruction': Sequence[Mapping[str, Union[str, float]]],\n",
    " 'edit_distance': float}\n",
    "```\n",
    "\n",
    "Field descriptions:\n",
    "\n",
    "*   `split`: The annotation split: `train`, `val_seen`, `val_unseen`,\n",
    "    `test_standard`.\n",
    "*   `instruction_id`: Uniquely identifies the guide annotation.\n",
    "*   `annotator_id`: Uniquely identifies the guide annotator.\n",
    "*   `language`: The IETF BCP 47 language tag: `en-IN`, `en-US`, `hi-IN`,\n",
    "    `te-IN`.\n",
    "*   `path_id`: Uniquely identifies a path sampled from the Matterport3D\n",
    "    environment.\n",
    "*   `scan`: Uniquely identifies a scan in the Matterport3D environment.\n",
    "*   `path`: A sequence of panoramic viewpoints along the path.\n",
    "*   `heading`: The initial heading in radians. Following R2R, the heading angle\n",
    "    is zero facing the y-axis with z-up, and increases by turning right.\n",
    "*   `instruction`: The navigation instruction.\n",
    "*   `timed_instruction`: A sequence of time-aligned words in the instruction.\n",
    "    Note that a small number of words are missing the `start_time` and\n",
    "    `end_time` fields.\n",
    "    *   `word`: The aligned utterance.\n",
    "    *   `start_time`: The start of the time span, w.r.t. the recording.\n",
    "    *   `end_time`: The end of the time span, w.r.t. the recording.\n",
    "*   `edit_distance` Edit distance between the manually transcribed instructions\n",
    "    and the automatic transcript generated by Google Cloud\n",
    "    [Text-to-Speech](https://cloud.google.com/text-to-speech) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las features de todos los puntos del dataset pesan 3.9G. Como no me cabe, tengo que tomar un sample\n",
    "DATA_SIZE_FACTOR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = './data/original'\n",
    "DATAPATHS = {\n",
    "    'train':        os.path.join(DATADIR, 'R2R_train.json'),\n",
    "    'test':         os.path.join(DATADIR, 'R2R_test.json'), \n",
    "    'val seen':     os.path.join(DATADIR, 'R2R_val_seen.json'),\n",
    "    'val unseen':   os.path.join(DATADIR, 'R2R_val_unseen.json'), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATAPATHS['train']) as f:\n",
    "    train = json.load(f)\n",
    "    train_df = pd.DataFrame.from_records(train)\n",
    "\n",
    "with open(DATAPATHS['test']) as f:\n",
    "    test = json.load(f)\n",
    "    test_df = pd.DataFrame.from_records(test)\n",
    "\n",
    "with open(DATAPATHS['val seen']) as f:\n",
    "    val_seen = json.load(f)\n",
    "    val_seen_df = pd.DataFrame.from_records(val_seen)\n",
    "\n",
    "with open(DATAPATHS['val unseen']) as f:\n",
    "    val_unseen = json.load(f)\n",
    "    val_unseen_df = pd.DataFrame.from_records(val_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 4675 ejemplos en el set de entrenamiento.\n",
      "Hay 1391 ejemplos en el set de test.\n",
      "Hay 340 ejemplos en el set de validacion seen.\n",
      "Hay 783 ejemplos en el set de validacion unseen.\n",
      "----------------------------------------------------\n",
      "Hay 61 escenarios distintos en el set de entrenamiento.\n",
      "Hay 18 escenarios distintos en el set de test.\n",
      "Hay 56 escenarios distintos en el set de validacion seen. (Todos extraidos de train)\n",
      "Hay 11 escenarios distintos en el set de validacion unseen.\n",
      "Hay 90 escenarios distintos en total.\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def scan_ids(df):\n",
    "    return df['scan'].unique()\n",
    "\n",
    "def n_scenarios(df):\n",
    "    return scan_ids(df).shape[0]\n",
    "\n",
    "print(f\"Hay {len(train)} ejemplos en el set de entrenamiento.\")\n",
    "print(f\"Hay {len(test)} ejemplos en el set de test.\")\n",
    "print(f\"Hay {len(val_seen)} ejemplos en el set de validacion seen.\")\n",
    "print(f\"Hay {len(val_unseen)} ejemplos en el set de validacion unseen.\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(f\"Hay {n_scenarios(train_df)} escenarios distintos en el set de entrenamiento.\")\n",
    "print(f\"Hay {n_scenarios(test_df)} escenarios distintos en el set de test.\")\n",
    "print(f\"Hay {n_scenarios(val_seen_df)} escenarios distintos en el set de validacion seen. (Todos extraidos de train)\")\n",
    "print(f\"Hay {n_scenarios(val_unseen_df)} escenarios distintos en el set de validacion unseen.\")\n",
    "print(f\"Hay {n_scenarios(pd.concat([train_df, test_df, val_seen_df, val_unseen_df]))} escenarios distintos en total.\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quedan 1 escenarios distintos en el set de entrenamiento.\n",
      "Quedan 1 escenarios distintos en el set de test.\n",
      "Quedan 1 escenarios distintos en el set de validacion seen. (Todos extraidos de train)\n",
      "Quedan 1 escenarios distintos en el set de validacion unseen.\n",
      "Quedan 3 escenarios distintos en total.\n"
     ]
    }
   ],
   "source": [
    "def sample_scan_ids(df):\n",
    "    unique_scan_ids = scan_ids(df)\n",
    "    return np.random.choice(unique_scan_ids, size=int(np.ceil(unique_scan_ids.shape[0] * DATA_SIZE_FACTOR)), replace=False)  # ceil hace que sea al menos 1.\n",
    "\n",
    "\n",
    "train_scan_ids      = sample_scan_ids(train_df)\n",
    "test_scan_ids       = sample_scan_ids(test_df)\n",
    "val_unseen_scan_ids = sample_scan_ids(val_unseen_df)\n",
    "val_seen_scan_ids   = np.intersect1d(train_scan_ids, scan_ids(val_seen_df))\n",
    "\n",
    "\n",
    "def filter_by_scan_id(split_dict, scans):\n",
    "    is_in_scans = lambda row: row['scan'] in scans\n",
    "    return list(filter(is_in_scans, split_dict))\n",
    "\n",
    "\n",
    "new_train      = filter_by_scan_id(train, train_scan_ids)\n",
    "new_test       = filter_by_scan_id(test, test_scan_ids)\n",
    "new_val_unseen = filter_by_scan_id(val_unseen, val_unseen_scan_ids)\n",
    "new_val_seen   = filter_by_scan_id(val_seen, val_seen_scan_ids)\n",
    "\n",
    "new_train_df      = pd.DataFrame.from_records(new_train)\n",
    "new_test_df       = pd.DataFrame.from_records(new_test)\n",
    "new_val_unseen_df = pd.DataFrame.from_records(new_val_unseen)\n",
    "new_val_seen_df   = pd.DataFrame.from_records(new_val_seen)\n",
    "\n",
    "# print(val_seen_scan_ids)\n",
    "\n",
    "print(f\"Quedan {n_scenarios(new_train_df)} escenarios distintos en el set de entrenamiento.\")\n",
    "print(f\"Quedan {n_scenarios(new_test_df)} escenarios distintos en el set de test.\")\n",
    "print(f\"Quedan {n_scenarios(new_val_seen_df)} escenarios distintos en el set de validacion seen. (Todos extraidos de train)\")\n",
    "print(f\"Quedan {n_scenarios(new_val_unseen_df)} escenarios distintos en el set de validacion unseen.\")\n",
    "print(f\"Quedan {n_scenarios(pd.concat([new_train_df, new_test_df, new_val_seen_df, new_val_unseen_df]))} escenarios distintos en total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los nuevos datasets reducidos\n",
    "\n",
    "DESTINY_DIR = 'data'\n",
    "DESTINY_PATHS = {\n",
    "    'train':        os.path.join(DESTINY_DIR, 'R2R_train.json'),\n",
    "    'test':         os.path.join(DESTINY_DIR, 'R2R_test.json'), \n",
    "    'val seen':     os.path.join(DESTINY_DIR, 'R2R_val_seen.json'),\n",
    "    'val unseen':   os.path.join(DESTINY_DIR, 'R2R_val_unseen.json'), \n",
    "}\n",
    "\n",
    "def save_json(obj, destiny_path):\n",
    "    with open(destiny_path, 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "save_json(new_train,      DESTINY_PATHS['train'])\n",
    "save_json(new_test,       DESTINY_PATHS['test'])\n",
    "save_json(new_val_seen,   DESTINY_PATHS['val seen'])\n",
    "save_json(new_val_unseen, DESTINY_PATHS['val unseen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los ids para luego filtrarlas features (en el runtime)\n",
    "all_scan_ids = np.hstack([\n",
    "    train_scan_ids,\n",
    "    test_scan_ids,\n",
    "    val_seen_scan_ids,\n",
    "    val_unseen_scan_ids\n",
    "])\n",
    "\n",
    "save_json(list(np.unique(all_scan_ids)), 'data/scan_ids.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
